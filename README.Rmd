---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

# maxcovr

[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/njtierney/maxcovr?branch=master&svg=true)](https://ci.appveyor.com/project/njtierney/maxcovr)[![Travis-CI Build Status](https://travis-ci.org/njtierney/maxcovr.svg?branch=master)](https://travis-ci.org/njtierney/maxcovr)

maxcovr provides tools to make it easy to solve the "maximum covering location problem", a binary optimisation problem described by [Church](http://www.geog.ucsb.edu/~forest/G294download/MAX_COVER_RLC_CSR.pdf). Currently it uses the `lp` solver from the `lpsolve` package.

# How to Install

```{r install, eval = FALSE}

# install.packages("devtools")
devtools::install_github("njtierney/maxcovr")
library(maxcovr)

```

<!-- # Example Usage -->
<!-- Need to find a good example dataset to use here -->

# Usage

We use the `york` data to obtain building locations, and then take a set of crime data from the [`ukpolice`  package](https://www.github.com/njtierney/ukpolice)

```{r example-usage}
library(maxcovr)
# install.packages("tidyverse")
library(tidyverse)
# install.packages("leaflet")
library(leaflet)
# devtools::install_github("njtierney/ukpolice")
library(ukpolice)


york %>%
    leaflet() %>%
    addTiles() %>%
    addCircleMarkers(popup = ~name)

```

```{r example-crime}

york_poly <- ukp_geo_chull(york, lat, long)

york_crime <- ukp_crime_poly(poly = york_poly)

york_crime %>%
    leaflet() %>%
    addTiles() %>%
    addCircles()

```

Say, for arguments sake, that we are interested in turning some listed buildings into police stations. We might be interested in those buildings that are within 200m of a crime location. We can use the `facility_user_dist()` function, which takes two dataframes with columns named "lat" and "long", and uses haverines formula to calculate the distance between them. At this stage both dataframes need to have the named columns "lat" and "long".

```{r example-dist}

york_distance <- facility_user_dist(facility = york,
                                    user = york_crime,
                                    coverage_distance = 200,
                                    nearest = "facility")

```

This does joins the york building dataset with the crime dataset and adds columns to identify the buildings and the crimes. The "nearest" option can be "facility", "user", and "both". Defaults to "facility". When set to "facility", returns a dataframe where every row is every crime, and the closest building to each crime. When set to "user", returns a dataframe where every row is every building, and the closest crime to each building. set to "both", which will return every pairwise combination of distances. Be careful whenDefault is "facility".

```{r example-head-dist}

head(york_distance)

```

The `coverage()` function calculates the coverage on the distance data frame. Here, we see that about 50%  of listed buildings are within 100m of a crime.

```{r example-coverage}

coverage(york_distance,
         dist_indic = 200,
         spread = TRUE)
```

You can also use `group_by` syntax from `dplyr` to get different information. Say for example, if you want to calculate the coverage for each crime category.

```{r example-group-by-cov}

york_distance %>%
    group_by(category) %>%
    coverage()

```

```{r}



```

# Known Issues

- `facility_user_dist()` is currently very slow, as it requires calculating the pairwise combination of every distance, and then subsetting based on what is closest. Future work is investigating faster, more sensible approaches.
- `max_coverage()` may take a bit of time to run, depending on your data size. If the product of your pairwise distance matrix exceeds 1 billion rows, it might take more than 1 minute.

`maxcovr` is still in beta, so there are likely to be unidentified bugs, please keep this in mind!

# Future Work

At the moment `max_coverage` uses the solver lpSolve, which is not as fast as other solvers such as CPLEX or Gurobi. In the future I will be providing alternative interfaces to other solvers, potentially using something like [`ompr`](https://github.com/dirkschumacher/ompr), to give users their own choice of solver, such as glpk, CPLEX, or Gurobi. A Gurobi interface is currently under development.

I also aim to provide functions that simplify the data common transformations (such as summaries of current coverage) when working with these kinds of problems, focussing on keeping the work in a dataframe, improvement in coverage, and more. Also on the cards are some standardized plots for exploration of data and results. If you have any suggestions, please file an issue and I will get to it as soon as I can.

# Code of Conduct

Please note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project you agree to abide by its terms.
